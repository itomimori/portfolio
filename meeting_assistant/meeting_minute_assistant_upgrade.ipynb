{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_osPeBQNAdv",
    "outputId": "11cc73e0-9aad-4f57-e1ae-2d71c4eb0444"
   },
   "outputs": [],
   "source": [
    "# Install required packages in Google Colab\n",
    "%pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2 gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL-8yTOlQiOH"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import threading\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "AUDIO_MODEL = \"whisper-1\"  # OpenAI Whisper API model\n",
    "LLM_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62c2Wbt3P5Ew"
   },
   "outputs": [],
   "source": [
    "# Google Colab User Data\n",
    "# Ensure you have set the following in your Google Colab environment:\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(hf_token, add_to_git_credential=True)\n",
    "openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smyocqu_P6yg"
   },
   "outputs": [],
   "source": [
    "class MeetingAssistant:\n",
    "    def __init__(self, model_name=LLM_MODEL, audio_model=AUDIO_MODEL):\n",
    "\n",
    "        # Load tokenizer and llm model\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        self.audio_model = audio_model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quant_config\n",
    "        )\n",
    "\n",
    "    def transcribe_audio(self, audio_path, progress):\n",
    "        \"\"\"Transcribes the uploaded audio file using OpenAI Whisper API.\"\"\"\n",
    "\n",
    "        progress(0.3, desc=\"Transcribing audio...\")\n",
    "\n",
    "        try:\n",
    "            with open(audio_path, \"rb\") as audio_file:\n",
    "                transcription = openai.audio.transcriptions.create(\n",
    "                    model=self.audio_model,\n",
    "                    file=audio_file,\n",
    "                    response_format=\"text\"\n",
    "                )\n",
    "                return transcription\n",
    "        except Exception as e:\n",
    "            return f\"Error during transcription: {str(e)}\"\n",
    "\n",
    "    def generate_minutes(self, transcription, progress):\n",
    "        \"\"\"Generates meeting minutes from the transcript using the Llama model.\"\"\"\n",
    "        progress(0.6, desc=\"Generating meeting minutes...\")\n",
    "\n",
    "        system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "        user_prompt = f\"Below is an extract transcript of a meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        streamer = TextIteratorStreamer(self.tokenizer)\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=self.model.generate, kwargs={\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"max_new_tokens\": 2000,\n",
    "                    \"streamer\": streamer\n",
    "                  })\n",
    "        thread.start()\n",
    "\n",
    "\n",
    "        started = False\n",
    "        # buffer = \"\"\n",
    "        for new_text in streamer:\n",
    "          if not started:\n",
    "              if \"<|start_header_id|>assistant<|end_header_id|>\" in new_text:\n",
    "                  started = True\n",
    "                  new_text = new_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "\n",
    "          if started:\n",
    "              if \"<|eot_id|>\" in new_text:\n",
    "                  new_text = new_text.replace(\"<|eot_id|>\", \"\")  # Remove the unwanted token\n",
    "\n",
    "              if new_text.strip():  # Only yield non-empty chunks\n",
    "                  yield new_text\n",
    "\n",
    "    def process_meeting(self, audio_file, progress):\n",
    "        \"\"\"Handles the complete process: transcribes audio and generates minutes.\"\"\"\n",
    "        progress(0.1, desc=\"Processing audio file...\")\n",
    "\n",
    "        # Check if a file is uploaded\n",
    "        if audio_file is None:\n",
    "            return \"Please upload an audio file.\"\n",
    "\n",
    "        try:\n",
    "          # Check file format\n",
    "          if not str(audio_file).lower().endswith('.mp3'):\n",
    "              return \"Please upload an MP3 file.\"\n",
    "\n",
    "          # Get transcription\n",
    "          transcription = self.transcribe_audio(audio_file, progress)\n",
    "\n",
    "          # Generate minutes\n",
    "          accumulated_text = \"\"\n",
    "          minutes = self.generate_minutes(transcription, progress)\n",
    "          for chunk in minutes:\n",
    "            accumulated_text += chunk  # Append new text\n",
    "            yield accumulated_text   # Update Gradio output with full text\n",
    "\n",
    "        except Exception as e:\n",
    "          return f\"Error processing file: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyMu9JrBRBGI"
   },
   "outputs": [],
   "source": [
    "class GradioInterface:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Gradio interface for processing audio files.\"\"\"\n",
    "        self.assistant = MeetingAssistant()\n",
    "        self.iface = gr.Interface(\n",
    "            fn=self.process_audio,\n",
    "            inputs=gr.Audio(type=\"filepath\", label=\"Upload MP3 File\", format=\"mp3\"),\n",
    "            outputs=gr.Markdown(label=\"Meeting Minutes\", min_height=60),\n",
    "            title=\"AI Meeting Assistant\",\n",
    "            description=\"Upload an audio file to transcribe and generate meeting minutes.\",\n",
    "            flagging_mode=\"never\"\n",
    "        )\n",
    "\n",
    "    def process_audio(self, audio_file, progress=gr.Progress()): # Adapter between the UI and the backend.\n",
    "        \"\"\"Handles user input from Gradio, processes the audio, and returns meeting minutes.\"\"\"\n",
    "        response = self.assistant.process_meeting(audio_file, progress)\n",
    "        for chunk in response:\n",
    "          yield chunk\n",
    "\n",
    "    def launch(self):\n",
    "        \"\"\"Launches the Gradio interface.\"\"\"\n",
    "        self.iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0f705a9046d34fb0a7ab8177a6521b88",
      "28e61ba2b56f4d3484dd3ec0eecb12aa",
      "ecb3207e61d44ca39c7f8a97546c6686",
      "5a4f5291a0b24178a44b1b3c2401a957",
      "2472dbee01b149c0ac7efe6eaa5ffd66",
      "65c6998bdf7444de85b37468f6b6f42e",
      "4323c3fbb5d24b38a920313479bb5c57",
      "8af3f0faa5b144efbf1aa443c2839d2c",
      "e0f3096904354279a95175d116816262",
      "0e6ad8796e9e4b868c8507722c9cbf33",
      "a32fde87feaf42199de6efe2c94085aa",
      "e6c97a25f41044ab89e49d9bf9836de4",
      "78562aef2a6a422dba1145306b294823",
      "980453816461473fab04be9b6fbf03b5",
      "39ce806000744113ae35a617adfe2571",
      "2991097320c148b7b0eb81e3ce866df2",
      "1a502c41fc3044c2a7c24ad144c209e9",
      "91a907a6aa044a3288cbf4deca77eb67",
      "7f78522bd56847bfa740ff8146e726d2",
      "eb2129af41b24e2ea64a962cb041164c",
      "44bdcc01d31a43eeaa7139018c24a83b",
      "a21477f95f604f618a3aa2f48c00f7f3",
      "0b97c1be64664458abfc0109857d86eb",
      "2d0a0c7b89a64b7499ce87e81044d461",
      "19acf6f364d8478ea264dde4fd4a1ca1",
      "c084a0f7a7c04c90a0fed54c60cc8e79",
      "482baf34221048bd8bb4e57cebe44707",
      "f9c94568e6b342dda23ccd3be906eec0",
      "c6f719622eae45b0b110b377918c2eb2",
      "83e68bf6b3994fd5a6eea4ba722864c3",
      "01d3ea10affb447ead36c6b4476e7a4c",
      "5f93b389541e4ec09aced45d018bc8c1",
      "90b2cfcc49804e78b6bebb383e9e6893",
      "6f1e02e1c1da4bd9a6d13b3907cd78ae",
      "39ea33000ee741c2b9fdf518f657d872",
      "1b6204edaebf489e9d3e70f6d722c33a",
      "4ae96d4297b84fd1a9022a9c07f7987e",
      "2c43cc66619945a18a82cd9437ea60be",
      "9e1dc2cc46fa4a4ea6c5d3a50333a02f",
      "7c11259f23a6440babc156ac7d4b94c7",
      "48c2c5afef3d47e3b9bada3cdc339ec9",
      "a0ca5ccd08df4b9191a6400907f239fa",
      "72a5d5d5f42e44f197b5829801fee49c",
      "37cefb5abc424fca84d5ec4d7b90ff1f",
      "afce35f6ca0545d99937a2fca8030cb0",
      "fab983c8f0d544a2950d03acd5c39644",
      "f1a00e2402d2498292cbc5b767b1b3a9",
      "ca4db027b9764a8180617aae1b215f60",
      "d96a8910fbc5451083df650386ce6726",
      "5bd8e043fbd64c7c9dfb0d871737786d",
      "1ca523532aa5433c91df9cb53291ba29",
      "0b5340eb370a490ea946a446a9ab2eaa",
      "5f822db8ca764ce4b8dd7b99a83c7286",
      "37622f8dcbe14ac5ad80c9b09c8c4005",
      "1a73f8a262a94cd48d0370bbfd582405",
      "99bfb965add64f609f0ef008c443cdb9",
      "f1afbbe6e1fa4239af3d79b42f1ffc26",
      "2574026b82a040f089bfd202db5ef91d",
      "32bc3c434f824c618659693de6bd929a",
      "6d19a5bd166443b1acbf261287be09ac",
      "58e73b94784645f699f957693aaf6e6e",
      "4e5c99b156c545b096ba538b1a8c588a",
      "bde0c4ad4eea4944b76b34ad9c19bb89",
      "b9ab6e3935c646e691c0b5143d47d4b3",
      "d4a06441bff74e0e9fe8978014660e90",
      "a8010eecf9bc4e8ebfc906489ff54543",
      "7e9cdfd05f074c1798b8e3d936f6e7de",
      "57b3fe293dda483bb3717d7bd3509cce",
      "b080c2078a3f4d93b4d8367755d96272",
      "a1a7f450bd8d4917b796c6e13a5be9e1",
      "e58ff13df5a04fd5b0496e82384fe439",
      "ddf5e150f83944bfb07d8f19a177a50a",
      "71939dd7929243e38419abec94b209f9",
      "fe17e6c350c54a2d85864bb8d6d50d85",
      "4a69b6ea437e4682819ed2d0aef048b8",
      "99a8e8cb1ad44d5f999c07cf9a913ef4",
      "1dd62e85589f4d60912e79dee1b39a3e",
      "0a634fec1cf544af82bd17af73bf417a",
      "c7e74bf1bb0f4d57ae95aa4397691e01",
      "e144ffa2b708446d940bfdf54741c7ab",
      "d65f1f5e345546b380c8e9be9d4dfb9b",
      "c2e4a8d768d245529bdac929585136c5",
      "a4317d864cc4445d8597ed695c3d4c35",
      "5429cebc5a28408985824c4f501e050e",
      "dc3100a6c9d946568ee0e297934773ff",
      "10ecb81d605a4534a13332371ac9041d",
      "570b18cdf9034cc780e838724a70904e",
      "1915b872ec55435080456092d9ec8717",
      "50f682e340cb4e58899aa7e9ea4741db",
      "569ec309d2f3490f94c85bbb3680258b",
      "dd319b7cb2b7425e849d6682c7f05390",
      "fde199a94cc7488690f81d5add9eb08d",
      "38d2575bf5144cf3a02d304444bdc481",
      "a06cc81bd0114aefb1e80868791b5be8",
      "819319dd58b44e999a13b0bd0e78c88e",
      "38f581406490488981c76bc7e7e64005",
      "2ef62c2bd93c462eb7c4522c8a156e0d",
      "c2cb2d57701a4e55b7bfa3f842a91c09",
      "4ba3375ddb584f068d2cdcf060cdfa9c",
      "125f8ae49e504b809e5e39f6d940204e",
      "2c4abe3e713846deb1b0a9bec03298d4",
      "b5f524e95a0d42febd4466bb7a8ad239",
      "b5b9a98cb74c409cbdc50a16a3393665",
      "ec145da746d74c5e896900f7462b630b",
      "aa7d2a5d452b4eb1b537542a9731b94a",
      "3d03ed01daeb43d58c4f15bc591043ba",
      "17b7619e5bfd4d74b1a3bee1c7643e74",
      "4c95edb35fab4e12b027248e93b61883",
      "7d7bcf713d3b4531846635fe43fb268e",
      "918a8b2b832645d18767bc2e4451d556",
      "3b8be978c3af4ef4b7bc16420e6a9f8a",
      "25efbd99d0134866940cc3fde41aacf7",
      "07e8ec9ab2ba4339a8e2736156f28eab",
      "7fa4e5411e384c568b37a51bc94b3ee2",
      "45fa63f56c814015861d05beb8800e09",
      "8a73f7fbe20a4d56a76588db2ac35cea",
      "3a18d0771ef74923ba209733afdd0e47",
      "9104c3953c254878b2020764569613a9",
      "8a237f2467734eddace5d9f9aafce9e7",
      "a737c2d22cd84141a2f28721ab69d28d",
      "be17f6c8dc9c40efa94c3af82a8efa6a",
      "7a74e712b53c4a659dc09766885c12d9",
      "9afb4b8b3ddf4a0abb75eedbcf3bc7c1",
      "27697007fa6d4736a5eb1e1b0eea2d82",
      "e5d4b0e78c3740cd8cf9cda0e4a93972",
      "2740d759410d435087c4ae0772d6ad73",
      "19f5f9369e3043e0984e160c50e0a32e",
      "7b488376756843fe84fcce2e7abb5cd9",
      "c2745572ac434351ad9b2c9506d8d0b7",
      "25119517d9a043ba91fd3fbefb8377a4",
      "4079ed7e7f794755afd5daad3f00a34a",
      "14f5761bfbd340198267e3986f4035a0"
     ]
    },
    "id": "BI91BBEJRB0K",
    "outputId": "c4853642-832e-4167-e220-2a2d0fd279a8"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = GradioInterface()\n",
    "    app.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
