{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20891,
     "status": "ok",
     "timestamp": 1757143932975,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "pR-ftUatjEGd",
    "outputId": "5df22f15-f1b0-4a06-ac8e-bf8207b62a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.0/308.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages in Google Colab\n",
    "%pip install -q python-dotenv gradio anthropic openai requests torch bitsandbytes transformers sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 35478,
     "status": "ok",
     "timestamp": 1757143970731,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "VPmk2-Ggi2Em"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import threading\n",
    "import anthropic\n",
    "import torch\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient, login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1757143974962,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "MiicxGawi2En"
   },
   "outputs": [],
   "source": [
    "# Google Colab User Data\n",
    "# Ensure you have set the following in your Google Colab environment:\n",
    "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1757143977122,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "zpUv8t71KATc"
   },
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "CODE_QWEN_URL = \"https://zfkokxzs1xrqv13v.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1757143980299,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "JgtqCyRji2En"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a specialized AI assistant for synthetic data generation. Your sole purpose is to create and save synthetic datasets based on a user's request.\n",
    "\n",
    "The user will provide a business problem and the desired output format (e.g., 'CSV', 'JSON'). You must generate a single, complete Python script that performs the following actions:\n",
    "\n",
    "1.  Use only standard Python libraries (e.g., `numpy`, `pandas`) and built-in libraries.\n",
    "2.  Generate a synthetic dataset that fits the described business problem.\n",
    "3.  Save the DataFrame to a file in the requested format.\n",
    "4.  When saving to JSON, use a standard `with open(...)` block to handle file encoding explicitly.\n",
    "5.  Print a confirmation message upon successful generation, including the name of the file created.\n",
    "\n",
    "Ensure all Python code blocks are correctly indented. The final output must contain **only** the executable Python code and no other text or explanation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757143982429,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "Bk6saP4oi2Eo"
   },
   "outputs": [],
   "source": [
    "def user_prompt(**input_data):\n",
    "  user_prompt = f\"\"\"\n",
    "      Generate a synthetic {input_data[\"dataset_type\"].lower()} dataset in {input_data[\"output_format\"].upper()} format.\n",
    "      Business problem: {input_data[\"business_problem\"]}\n",
    "      Samples: {input_data[\"num_samples\"]}\n",
    "      \"\"\"\n",
    "  return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757143987389,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "Sx7hHKczi2Eo"
   },
   "outputs": [],
   "source": [
    "def stream_gpt(user_prompt):\n",
    "  stream = openai.chat.completions.create(\n",
    "      model=OPENAI_MODEL,\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": system_message},\n",
    "          {\"role\": \"user\",\"content\": user_prompt},\n",
    "      ],\n",
    "      stream=True,\n",
    "  )\n",
    "\n",
    "  response = \"\"\n",
    "  for chunk in stream:\n",
    "      response += chunk.choices[0].delta.content or \"\"\n",
    "      yield response\n",
    "\n",
    "  return response\n",
    "\n",
    "\n",
    "def stream_claude(user_prompt):\n",
    "  result = claude.messages.stream(\n",
    "      model=CLAUDE_MODEL,\n",
    "      max_tokens=2000,\n",
    "      system=system_message,\n",
    "      messages=[\n",
    "          {\"role\": \"user\",\"content\": user_prompt}\n",
    "      ]\n",
    "  )\n",
    "  reply = \"\"\n",
    "  with result as stream:\n",
    "      for text in stream.text_stream:\n",
    "          reply += text\n",
    "          yield reply\n",
    "          print(text, end=\"\", flush=True)\n",
    "  return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1757143991976,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "W0AuZT2uk0Sd"
   },
   "outputs": [],
   "source": [
    "def stream_llama(user_prompt):\n",
    "  try:\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\",\"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLAMA,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs={\n",
    "        \"input_ids\": inputs,\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"streamer\": streamer\n",
    "    })\n",
    "    thread.start()\n",
    "\n",
    "    started = False\n",
    "    reply = \"\"\n",
    "\n",
    "    for new_text in streamer:\n",
    "        if not started:\n",
    "            if \"<|start_header_id|>assistant<|end_header_id|>\" in new_text:\n",
    "                started = True\n",
    "                new_text = new_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if \"<|eot_id|>\" in new_text:\n",
    "            new_text = new_text.replace(\"<|eot_id|>\", \"\")\n",
    "            if new_text.strip():\n",
    "                reply += new_text\n",
    "                yield reply\n",
    "            break\n",
    "\n",
    "        if new_text.strip():\n",
    "            reply += new_text\n",
    "            yield reply\n",
    "\n",
    "    return reply\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"LLaMA error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757143994498,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "V0JS_6THi2Eo"
   },
   "outputs": [],
   "source": [
    "def stream_code_qwen(user_prompt):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\",\"content\": user_prompt},\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    client = InferenceClient(CODE_QWEN_URL, token=hf_token)\n",
    "    stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757143995669,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "YqSKnklRi2Eo"
   },
   "outputs": [],
   "source": [
    "def generate_from_inputs(model, **input_data):\n",
    "  # print(\"ğŸ” input_data received:\", input_data)\n",
    "  user_prompt_str = user_prompt(**input_data)\n",
    "\n",
    "  if model == \"GPT\":\n",
    "    result = stream_gpt(user_prompt_str)\n",
    "  elif model == \"Claude\":\n",
    "    result = stream_claude(user_prompt_str)\n",
    "  elif model == \"Llama\":\n",
    "    result = stream_llama(user_prompt_str)\n",
    "  elif model == \"Code Qwen\":\n",
    "    result = stream_code_qwen(user_prompt_str)\n",
    "  else:\n",
    "    raise ValueError(\"Unknown model\")\n",
    "\n",
    "  for stream_so_far in result:\n",
    "    yield stream_so_far\n",
    "\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757143998385,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "zG6_TSfni2Eo"
   },
   "outputs": [],
   "source": [
    "def handle_generate(business_problem, dataset_type, dataset_format, num_samples, model):\n",
    "  input_data = {\n",
    "      \"business_problem\": business_problem,\n",
    "      \"dataset_type\": dataset_type,\n",
    "      \"output_format\": dataset_format,\n",
    "      \"num_samples\": num_samples,\n",
    "  }\n",
    "\n",
    "  response = generate_from_inputs(model, **input_data)\n",
    "  for chunk in response:\n",
    "      yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1757144000947,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "NcEkmsnai2Ep",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_code(text):\n",
    "  match = re.search(r\"```python(.*?)```\", text, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "      code = match.group(0).strip()\n",
    "  else:\n",
    "      code = \"\"\n",
    "      print(\"No matching substring found.\")\n",
    "\n",
    "  return code.replace(\"```python\\n\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "\n",
    "def execute_code_in_virtualenv(text, python_interpreter=sys.executable):\n",
    "  if not python_interpreter:\n",
    "      raise EnvironmentError(\"Python interpreter not found in the specified virtual environment.\")\n",
    "\n",
    "  code_str = extract_code(text)\n",
    "  command = [python_interpreter, '-c', code_str]\n",
    "\n",
    "  try:\n",
    "      result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "      stdout = result.stdout\n",
    "      return stdout\n",
    "\n",
    "  except subprocess.CalledProcessError as e:\n",
    "      return f\"Execution error:\\n{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1757144008334,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "SEiZVkdFi2Ep"
   },
   "outputs": [],
   "source": [
    "def update_output_format(dataset_type):\n",
    "    if dataset_type in [\"Tabular\", \"Time-series\"]:\n",
    "        return gr.update(choices=[\"JSON\", \"csv\"], value=\"JSON\")\n",
    "    elif dataset_type == \"Text\":\n",
    "        return gr.update(choices=[\"JSON\"], value=\"JSON\")\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"## Create a dataset for a business problem\")\n",
    "\n",
    "    with gr.Column():\n",
    "        business_problem = gr.Textbox(label=\"Business problem\", lines=2)\n",
    "        dataset_type = gr.Dropdown(\n",
    "            [\"Tabular\", \"Time-series\", \"Text\"], label=\"Dataset type\"\n",
    "        )\n",
    "\n",
    "        output_format = gr.Dropdown( choices=[\"JSON\", \"csv\"], value=\"JSON\",label=\"Output Format\")\n",
    "\n",
    "        num_samples = gr.Number(label=\"Number of samples\", value=10, precision=0)\n",
    "\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\", \"Llama\", \"Code Qwen\"], label=\"Select model\", value=\"GPT\")\n",
    "\n",
    "        dataset_type.change(update_output_format,inputs=[dataset_type], outputs=[output_format])\n",
    "\n",
    "    with gr.Row():\n",
    "            with gr.Column():\n",
    "                dataset_run = gr.Button(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\")\n",
    "                gr.Markdown(\"\"\"âš ï¸ Llama ã‚„ Code Qwen ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ç”Ÿæˆã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰ã¯æœ€é©ã§ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "                              å®Ÿè¡Œå‰ã«å†…å®¹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚èª¤ã‚ŠãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\"\"\")\n",
    "\n",
    "            with gr.Column():\n",
    "              code_run = gr.Button(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\")\n",
    "              gr.Markdown(\"\"\"âš ï¸ ã“ã®ã‚¢ãƒ—ãƒªã‚’ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œä»˜ãã§å…¬é–‹ã™ã‚‹éš›ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚\n",
    "                            ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã¯ã€æ½œåœ¨çš„ãªè„†å¼±æ€§ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€\n",
    "                            æœ¬ãƒ„ãƒ¼ãƒ«ã®åˆ©ç”¨ã¯è²¬ä»»ã‚’æŒã£ã¦è¡Œã£ã¦ãã ã•ã„ã€‚\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        dataset_out = gr.Textbox(label=\"Generated Dataset\")\n",
    "        code_out = gr.Textbox(label=\"Executed code\")\n",
    "\n",
    "    dataset_run.click(\n",
    "        handle_generate,\n",
    "        inputs=[business_problem, dataset_type, output_format, num_samples, model],\n",
    "        outputs=[dataset_out]\n",
    "    )\n",
    "\n",
    "    code_run.click(\n",
    "        execute_code_in_virtualenv,\n",
    "        inputs=[dataset_out],\n",
    "        outputs=[code_out]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "executionInfo": {
     "elapsed": 3063,
     "status": "ok",
     "timestamp": 1757144012463,
     "user": {
      "displayName": "å†¨æ£®ä¸€æ±°",
      "userId": "04293913759182540680"
     },
     "user_tz": -540
    },
    "id": "jCAkTEtMi2Ep",
    "outputId": "6fa6e623-65f0-4556-852d-e8fead568cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://a98162bd476efce801.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a98162bd476efce801.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
